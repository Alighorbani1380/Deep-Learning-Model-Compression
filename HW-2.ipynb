{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# data prepration"
   ],
   "metadata": {
    "id": "V33XailwJdzH"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gCSGhyRIJXhi",
    "ExecuteTime": {
     "end_time": "2024-11-16T10:38:21.115021Z",
     "start_time": "2024-11-16T10:38:21.110457Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sympy.physics.units import amount\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4s6PTpidJXhp",
    "ExecuteTime": {
     "end_time": "2024-11-16T10:38:25.309894Z",
     "start_time": "2024-11-16T10:38:25.279048Z"
    }
   },
   "source": [
    "# LOAD\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "yzSUEGUaJXhq",
    "outputId": "ddde4e79-6c76-410d-87de-d65527a5186c",
    "ExecuteTime": {
     "end_time": "2024-11-16T07:18:56.966922Z",
     "start_time": "2024-11-16T07:18:56.531401Z"
    }
   },
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training"
   ],
   "metadata": {
    "id": "mB4gLpB3jjpY"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ftcCGnh6JXht",
    "ExecuteTime": {
     "end_time": "2024-11-16T07:21:07.237178Z",
     "start_time": "2024-11-16T07:21:07.229451Z"
    }
   },
   "source": [
    "# model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        #self.relu = nn.ReLU(),\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #self.relu = nn.ReLU(),\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 1000)  # Adjust for output size after pooling\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(1000, 120)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.bn1(torch.relu(self.conv1(x))))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.pool(self.bn2(torch.relu(self.conv2(x))))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.pool(self.bn3(torch.relu(self.conv3(x))))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.view(-1, 128 * 3 * 3)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "model = FashionMNISTModel()\n",
    "print(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88ZD8GP0cC1c",
    "outputId": "ba8875c5-f402-4e08-d9c1-ee556e8b8135",
    "ExecuteTime": {
     "end_time": "2024-11-16T10:38:33.394089Z",
     "start_time": "2024-11-16T10:38:33.379259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNISTModel(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1152, out_features=1000, bias=True)\n",
      "  (drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=1000, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "source": [
    "#train\n",
    "model = FashionMNISTModel()\n",
    "criterion = nn.CrossEntropyLoss() # loss function \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9) # optimizer\n",
    "\n",
    "#defining training hyperparameters and cuda setting \n",
    "num_epochs = 40\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i,data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) #passing forward the inputs\n",
    "        loss = criterion(outputs, labels) # calculating loss function \n",
    "        loss.backward() # Backpropagation the loss\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print(f\"[Epoch {epoch + 1}, Mini-batch {i + 1}] Loss: {running_loss / 200:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "torch.save(model.state_dict(), \"fashion_mnist_cnn40.pth\")\n",
    "print(\"training finished and saved\")\n"
   ],
   "metadata": {
    "id": "wefZRqTYtjia",
    "ExecuteTime": {
     "end_time": "2024-11-15T12:13:24.359345Z",
     "start_time": "2024-11-15T12:01:42.767937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Mini-batch 200] Loss: 1.179\n",
      "[Epoch 1, Mini-batch 400] Loss: 0.703\n",
      "[Epoch 1, Mini-batch 600] Loss: 0.615\n",
      "[Epoch 1, Mini-batch 800] Loss: 0.533\n",
      "[Epoch 2, Mini-batch 200] Loss: 0.521\n",
      "[Epoch 2, Mini-batch 400] Loss: 0.505\n",
      "[Epoch 2, Mini-batch 600] Loss: 0.481\n",
      "[Epoch 2, Mini-batch 800] Loss: 0.483\n",
      "[Epoch 3, Mini-batch 200] Loss: 0.454\n",
      "[Epoch 3, Mini-batch 400] Loss: 0.464\n",
      "[Epoch 3, Mini-batch 600] Loss: 0.439\n",
      "[Epoch 3, Mini-batch 800] Loss: 0.457\n",
      "[Epoch 4, Mini-batch 200] Loss: 0.449\n",
      "[Epoch 4, Mini-batch 400] Loss: 0.467\n",
      "[Epoch 4, Mini-batch 600] Loss: 0.442\n",
      "[Epoch 4, Mini-batch 800] Loss: 0.437\n",
      "[Epoch 5, Mini-batch 200] Loss: 0.437\n",
      "[Epoch 5, Mini-batch 400] Loss: 0.451\n",
      "[Epoch 5, Mini-batch 600] Loss: 0.431\n",
      "[Epoch 5, Mini-batch 800] Loss: 0.441\n",
      "[Epoch 6, Mini-batch 200] Loss: 0.425\n",
      "[Epoch 6, Mini-batch 400] Loss: 0.424\n",
      "[Epoch 6, Mini-batch 600] Loss: 0.436\n",
      "[Epoch 6, Mini-batch 800] Loss: 0.416\n",
      "[Epoch 7, Mini-batch 200] Loss: 0.414\n",
      "[Epoch 7, Mini-batch 400] Loss: 0.423\n",
      "[Epoch 7, Mini-batch 600] Loss: 0.416\n",
      "[Epoch 7, Mini-batch 800] Loss: 0.405\n",
      "[Epoch 8, Mini-batch 200] Loss: 0.436\n",
      "[Epoch 8, Mini-batch 400] Loss: 0.420\n",
      "[Epoch 8, Mini-batch 600] Loss: 0.408\n",
      "[Epoch 8, Mini-batch 800] Loss: 0.390\n",
      "[Epoch 9, Mini-batch 200] Loss: 0.389\n",
      "[Epoch 9, Mini-batch 400] Loss: 0.415\n",
      "[Epoch 9, Mini-batch 600] Loss: 0.388\n",
      "[Epoch 9, Mini-batch 800] Loss: 0.391\n",
      "[Epoch 10, Mini-batch 200] Loss: 0.378\n",
      "[Epoch 10, Mini-batch 400] Loss: 0.413\n",
      "[Epoch 10, Mini-batch 600] Loss: 0.389\n",
      "[Epoch 10, Mini-batch 800] Loss: 0.370\n",
      "[Epoch 11, Mini-batch 200] Loss: 0.387\n",
      "[Epoch 11, Mini-batch 400] Loss: 0.389\n",
      "[Epoch 11, Mini-batch 600] Loss: 0.373\n",
      "[Epoch 11, Mini-batch 800] Loss: 0.355\n",
      "[Epoch 12, Mini-batch 200] Loss: 0.377\n",
      "[Epoch 12, Mini-batch 400] Loss: 0.379\n",
      "[Epoch 12, Mini-batch 600] Loss: 0.360\n",
      "[Epoch 12, Mini-batch 800] Loss: 0.365\n",
      "[Epoch 13, Mini-batch 200] Loss: 0.367\n",
      "[Epoch 13, Mini-batch 400] Loss: 0.364\n",
      "[Epoch 13, Mini-batch 600] Loss: 0.348\n",
      "[Epoch 13, Mini-batch 800] Loss: 0.355\n",
      "[Epoch 14, Mini-batch 200] Loss: 0.336\n",
      "[Epoch 14, Mini-batch 400] Loss: 0.339\n",
      "[Epoch 14, Mini-batch 600] Loss: 0.328\n",
      "[Epoch 14, Mini-batch 800] Loss: 0.345\n",
      "[Epoch 15, Mini-batch 200] Loss: 0.349\n",
      "[Epoch 15, Mini-batch 400] Loss: 0.344\n",
      "[Epoch 15, Mini-batch 600] Loss: 0.331\n",
      "[Epoch 15, Mini-batch 800] Loss: 0.331\n",
      "[Epoch 16, Mini-batch 200] Loss: 0.326\n",
      "[Epoch 16, Mini-batch 400] Loss: 0.328\n",
      "[Epoch 16, Mini-batch 600] Loss: 0.312\n",
      "[Epoch 16, Mini-batch 800] Loss: 0.331\n",
      "[Epoch 17, Mini-batch 200] Loss: 0.320\n",
      "[Epoch 17, Mini-batch 400] Loss: 0.321\n",
      "[Epoch 17, Mini-batch 600] Loss: 0.318\n",
      "[Epoch 17, Mini-batch 800] Loss: 0.315\n",
      "[Epoch 18, Mini-batch 200] Loss: 0.321\n",
      "[Epoch 18, Mini-batch 400] Loss: 0.318\n",
      "[Epoch 18, Mini-batch 600] Loss: 0.319\n",
      "[Epoch 18, Mini-batch 800] Loss: 0.304\n",
      "[Epoch 19, Mini-batch 200] Loss: 0.325\n",
      "[Epoch 19, Mini-batch 400] Loss: 0.322\n",
      "[Epoch 19, Mini-batch 600] Loss: 0.309\n",
      "[Epoch 19, Mini-batch 800] Loss: 0.336\n",
      "[Epoch 20, Mini-batch 200] Loss: 0.306\n",
      "[Epoch 20, Mini-batch 400] Loss: 0.314\n",
      "[Epoch 20, Mini-batch 600] Loss: 0.326\n",
      "[Epoch 20, Mini-batch 800] Loss: 0.323\n",
      "[Epoch 21, Mini-batch 200] Loss: 0.307\n",
      "[Epoch 21, Mini-batch 400] Loss: 0.316\n",
      "[Epoch 21, Mini-batch 600] Loss: 0.297\n",
      "[Epoch 21, Mini-batch 800] Loss: 0.324\n",
      "[Epoch 22, Mini-batch 200] Loss: 0.303\n",
      "[Epoch 22, Mini-batch 400] Loss: 0.301\n",
      "[Epoch 22, Mini-batch 600] Loss: 0.305\n",
      "[Epoch 22, Mini-batch 800] Loss: 0.317\n",
      "[Epoch 23, Mini-batch 200] Loss: 0.298\n",
      "[Epoch 23, Mini-batch 400] Loss: 0.309\n",
      "[Epoch 23, Mini-batch 600] Loss: 0.298\n",
      "[Epoch 23, Mini-batch 800] Loss: 0.299\n",
      "[Epoch 24, Mini-batch 200] Loss: 0.301\n",
      "[Epoch 24, Mini-batch 400] Loss: 0.310\n",
      "[Epoch 24, Mini-batch 600] Loss: 0.306\n",
      "[Epoch 24, Mini-batch 800] Loss: 0.305\n",
      "[Epoch 25, Mini-batch 200] Loss: 0.308\n",
      "[Epoch 25, Mini-batch 400] Loss: 0.294\n",
      "[Epoch 25, Mini-batch 600] Loss: 0.305\n",
      "[Epoch 25, Mini-batch 800] Loss: 0.298\n",
      "[Epoch 26, Mini-batch 200] Loss: 0.285\n",
      "[Epoch 26, Mini-batch 400] Loss: 0.316\n",
      "[Epoch 26, Mini-batch 600] Loss: 0.335\n",
      "[Epoch 26, Mini-batch 800] Loss: 0.302\n",
      "[Epoch 27, Mini-batch 200] Loss: 0.293\n",
      "[Epoch 27, Mini-batch 400] Loss: 0.302\n",
      "[Epoch 27, Mini-batch 600] Loss: 0.303\n",
      "[Epoch 27, Mini-batch 800] Loss: 0.304\n",
      "[Epoch 28, Mini-batch 200] Loss: 0.294\n",
      "[Epoch 28, Mini-batch 400] Loss: 0.311\n",
      "[Epoch 28, Mini-batch 600] Loss: 0.305\n",
      "[Epoch 28, Mini-batch 800] Loss: 0.310\n",
      "[Epoch 29, Mini-batch 200] Loss: 0.328\n",
      "[Epoch 29, Mini-batch 400] Loss: 0.310\n",
      "[Epoch 29, Mini-batch 600] Loss: 0.305\n",
      "[Epoch 29, Mini-batch 800] Loss: 0.305\n",
      "[Epoch 30, Mini-batch 200] Loss: 0.308\n",
      "[Epoch 30, Mini-batch 400] Loss: 0.312\n",
      "[Epoch 30, Mini-batch 600] Loss: 0.290\n",
      "[Epoch 30, Mini-batch 800] Loss: 0.283\n",
      "[Epoch 31, Mini-batch 200] Loss: 0.286\n",
      "[Epoch 31, Mini-batch 400] Loss: 0.285\n",
      "[Epoch 31, Mini-batch 600] Loss: 0.288\n",
      "[Epoch 31, Mini-batch 800] Loss: 0.282\n",
      "[Epoch 32, Mini-batch 200] Loss: 0.300\n",
      "[Epoch 32, Mini-batch 400] Loss: 0.295\n",
      "[Epoch 32, Mini-batch 600] Loss: 0.311\n",
      "[Epoch 32, Mini-batch 800] Loss: 0.300\n",
      "[Epoch 33, Mini-batch 200] Loss: 0.309\n",
      "[Epoch 33, Mini-batch 400] Loss: 0.284\n",
      "[Epoch 33, Mini-batch 600] Loss: 0.294\n",
      "[Epoch 33, Mini-batch 800] Loss: 0.308\n",
      "[Epoch 34, Mini-batch 200] Loss: 0.305\n",
      "[Epoch 34, Mini-batch 400] Loss: 0.306\n",
      "[Epoch 34, Mini-batch 600] Loss: 0.287\n",
      "[Epoch 34, Mini-batch 800] Loss: 0.307\n",
      "[Epoch 35, Mini-batch 200] Loss: 0.305\n",
      "[Epoch 35, Mini-batch 400] Loss: 0.313\n",
      "[Epoch 35, Mini-batch 600] Loss: 0.307\n",
      "[Epoch 35, Mini-batch 800] Loss: 0.282\n",
      "[Epoch 36, Mini-batch 200] Loss: 0.302\n",
      "[Epoch 36, Mini-batch 400] Loss: 0.292\n",
      "[Epoch 36, Mini-batch 600] Loss: 0.286\n",
      "[Epoch 36, Mini-batch 800] Loss: 0.277\n",
      "[Epoch 37, Mini-batch 200] Loss: 0.283\n",
      "[Epoch 37, Mini-batch 400] Loss: 0.292\n",
      "[Epoch 37, Mini-batch 600] Loss: 0.284\n",
      "[Epoch 37, Mini-batch 800] Loss: 0.285\n",
      "[Epoch 38, Mini-batch 200] Loss: 0.293\n",
      "[Epoch 38, Mini-batch 400] Loss: 0.285\n",
      "[Epoch 38, Mini-batch 600] Loss: 0.297\n",
      "[Epoch 38, Mini-batch 800] Loss: 0.294\n",
      "[Epoch 39, Mini-batch 200] Loss: 0.290\n",
      "[Epoch 39, Mini-batch 400] Loss: 0.283\n",
      "[Epoch 39, Mini-batch 600] Loss: 0.292\n",
      "[Epoch 39, Mini-batch 800] Loss: 0.281\n",
      "[Epoch 40, Mini-batch 200] Loss: 0.278\n",
      "[Epoch 40, Mini-batch 400] Loss: 0.281\n",
      "[Epoch 40, Mini-batch 600] Loss: 0.280\n",
      "[Epoch 40, Mini-batch 800] Loss: 0.285\n",
      "training finished and saved\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# reload the model ( for debug)\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:09:00.980675Z",
     "start_time": "2024-11-16T14:09:00.948303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FashionMNISTModel()\n",
    "model.load_state_dict(torch.load(\"/home/oem/Desktop/uni/ut/fashion_mnist_cnn.pth\"))\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device) "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1152, out_features=1000, bias=True)\n",
       "  (drop): Dropout2d(p=0.25, inplace=False)\n",
       "  (fc2): Linear(in_features=1000, out_features=120, bias=True)\n",
       "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ziVMA-n9JXhw",
    "ExecuteTime": {
     "end_time": "2024-11-16T14:09:21.112999Z",
     "start_time": "2024-11-16T14:09:05.472057Z"
    }
   },
   "source": [
    "# Test\n",
    "\n",
    "\n",
    "model.eval()  \n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during testing\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device) \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy test images: {100 * correct / total:.2f}%\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation during testing\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device) \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy train images: {100 * correct / total:.2f}%\")\n",
    "# Class-wise accuracy (optional)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test images: 93.14%\n",
      "Accuracy train images: 96.42%\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "id": "aiPQuJBKKDY9"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), '/home/oem/Desktop/uni/ut/acc92.2200pruned0.40.pth')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  reporting based on sklearn\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T10:05:09.752452Z",
     "start_time": "2024-11-15T10:05:05.618371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())  # Move predictions to CPU and convert to NumPy\n",
    "        all_labels.extend(labels.cpu().numpy())  # Move labels to CPU and convert to NumPy\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(all_labels, all_predictions, target_names=labels_map.values())\n",
    "print(report)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     T-Shirt       0.89      0.88      0.88      1000\n",
      "     Trouser       0.99      0.99      0.99      1000\n",
      "    Pullover       0.88      0.92      0.90      1000\n",
      "       Dress       0.93      0.94      0.94      1000\n",
      "        Coat       0.90      0.87      0.89      1000\n",
      "      Sandal       0.99      0.97      0.98      1000\n",
      "       Shirt       0.80      0.79      0.79      1000\n",
      "     Sneaker       0.95      0.99      0.97      1000\n",
      "         Bag       0.99      0.99      0.99      1000\n",
      "  Ankle Boot       0.99      0.97      0.98      1000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# گزارش نتیجه بدست آمده از بخش آموزش و تست \n",
    " ضریب recall بیانگرنسبت tp به مجموع tp و fn است که نشان دهنده حساسیت مدل روی هر دسته از داده هایی است که موجود داریم \n",
    "ضریب precision بیانگر نسبت tp  به مجموع tp به fp است که بیانگر دقت تشخیص درست مدل ما از بین نمونه هایی که به عنوان هدف درست انتخاب شده اند , است . \n",
    "ضریب f1-score بیانگر میانگین بین ضرایب recall و precision است .  این ضریب دید کلی و مناسبی از دقت و کارایی مدل به ما میدهد . \n",
    "در این گزارش هر سه پارامتر به ازای هر کلاس از دیتاست بررسی شده و گزارش گردیده است .\n",
    "با بررسی نتایج بدست آمده از این گزارش به این نتیجه میرسیم که  با علم بر این که دقت کلی مدل برابر با ۹۴ درصد است, در اکثر کلاس ها مقدار f1-score بالای ۹۰ درصد بوده و تفاوت اندکی با دقت کلی مدل دارد اما در بعضی از کلاس ها مانند T-Shirt  و  Shirt این نسبت کمتر از ۹۰ است و به بررسی دلیل مسئله میپردازیم .\n",
    "بررسی نتیجه ضرایب Shirt : \n",
    "مقدار ضریب recall این دسته از داده ها با precision  آن برابر بوده (۰.۸۲) و این نشان دهنده آن است که مدل در این دسته بندی در تعادلی از خطا در مقادیر fp و fn  در امر شناسایی این دسته از داده است \n",
    "\n",
    "بررسی نتیجه ضرایب T_shirt :\n",
    "دسته T_shirt دارای precisionبرابر با  0.90 ومقدار  0.88 به عنوان recallاست . این ضرایب نشان دهنده FN کمی بیشتر از FP است. این نشان می‌دهد که مدل در شناسایی درست T_shirt های واقعی در هنگام پیش‌بینی آن‌ها دقت لازم را دارد، اما بخش کمی بزرگ‌تر از T_shirt واقعی را در مجموعه داده‌ها از دست می‌دهد. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# L1 unstructured pruning",
   "metadata": {
    "id": "unfyz5RSJjS5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "id": "HLuByxUGQf1Z",
    "ExecuteTime": {
     "end_time": "2024-11-16T07:29:06.736648Z",
     "start_time": "2024-11-16T07:29:06.731432Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## one-shot"
   ],
   "metadata": {
    "id": "1lWRcQBOnqov"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oTV0D5iMJXhx",
    "ExecuteTime": {
     "end_time": "2024-11-16T07:29:08.731903Z",
     "start_time": "2024-11-16T07:29:08.728646Z"
    }
   },
   "source": [
    "# Function to apply L1 unstructured pruning\n",
    "def apply_pruning(model, amount):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "\n",
    "    return model\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T09:19:05.006698Z",
     "start_time": "2024-11-16T09:19:02.642628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reloading the model (tmp)\n",
    "model = FashionMNISTModel()\n",
    "model.load_state_dict(torch.load(\"fashion_mnist_cnn.pth\"))\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device) \n",
    "#print(model)\n",
    "model.eval()  \n",
    "# testing model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient calculation during testing\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device) \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy test images: {100 * correct / total:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test images: 93.15%\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To check sparsity level in layers (tmp)\n",
    "model = apply_pruning(model, 0.35)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# one-step pruning"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:45:07.588383Z",
     "start_time": "2024-11-16T13:44:54.358548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# one-step pruning\n",
    "\n",
    "max_acc = 92.15\n",
    "fix_mount = 0\n",
    "least_it = 10\n",
    "for  it in range(10,101,5):\n",
    "    it = it/100\n",
    "    print('checking amount= ',it)\n",
    "    # pruning iteration \n",
    "    model = apply_pruning(model, it)\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation during testing\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy test images: {100 * correct / total:.2f}%\\n\")\n",
    "    acc = 100 * correct / total\n",
    "    if acc > 92.15:\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            fix_mount = it\n",
    "    if acc < 92 :\n",
    "        print(f\"the mount = {it:.2f} was exceeding the 1% accuracy loss threshold \")\n",
    "        break\n",
    "    # reloading the model\n",
    "    model = FashionMNISTModel()\n",
    "    model.load_state_dict(torch.load(\"fashion_mnist_cnn.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking amount=  0.1\n",
      "Accuracy test images: 93.17%\n",
      "\n",
      "checking amount=  0.15\n",
      "Accuracy test images: 92.94%\n",
      "\n",
      "checking amount=  0.2\n",
      "Accuracy test images: 93.07%\n",
      "\n",
      "checking amount=  0.25\n",
      "Accuracy test images: 92.83%\n",
      "\n",
      "checking amount=  0.3\n",
      "Accuracy test images: 92.69%\n",
      "\n",
      "checking amount=  0.35\n",
      "Accuracy test images: 91.33%\n",
      "\n",
      "the mount = 0.35 was exceeding the 1% accuracy loss threshold \n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# step by step pruning"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:45:27.740959Z",
     "start_time": "2024-11-16T13:45:27.735989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check sparsity \n",
    "def check_sparsity(model):\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):  # Check for layers with weights\n",
    "            weight = module.weight.data\n",
    "            zero_count = torch.sum(weight == 0)\n",
    "            total_count = torch.numel(weight)  # Total number of elements in the weight tensor\n",
    "            sparsity = 100. * zero_count / total_count\n",
    "            print(f\"Sparsity in {name}.weight: {sparsity:.2f}%\")\n"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "id": "gccY3LbFn9xX",
    "ExecuteTime": {
     "end_time": "2024-11-16T15:10:32.170877Z",
     "start_time": "2024-11-16T14:09:44.567617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#prun & finetune the pruned model\n",
    "best_accuracy = 0.9314  # Original accuracy\n",
    "accuracy_threshold = 0.01 # Acceptable accuracy loss\n",
    "prune_amount = 0.1\n",
    "prune_step = 0.05\n",
    "best_prune_amount = 0\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, device, labels_map ,prune_amount):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # You might need to adjust these\n",
    "    epochs = 50 # Adjust as needed\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Accuracy of the network on test images after Pruning: {accuracy:.4f}%\")\n",
    "    torch.save(model.state_dict(), f\"./pruned_results/acc{accuracy:.4f}pruned{prune_amount:.2f}.pth\")\n",
    "    check_sparsity(model)\n",
    "\n",
    "    return accuracy/100\n",
    "\n",
    "while True:\n",
    "    print(f\"\\nPruning with amount: {prune_amount:.2f}\")\n",
    "\n",
    "    pruned_model = apply_pruning(model, prune_amount)  # Create a copy if you need the original model\n",
    "    pruned_model.to(device)\n",
    "    current_accuracy = train_and_evaluate(pruned_model,train_loader,test_loader,device,labels_map,prune_amount)\n",
    "\n",
    "    if (best_accuracy - current_accuracy) < accuracy_threshold:\n",
    "        best_prune_amount = prune_amount\n",
    "        prune_amount += prune_step\n",
    "        model = pruned_model.cpu().to(device) # update the model for the next iteration with pruning\n",
    "    else:\n",
    "        print(f\"Accuracy dropped below threshold. Best prune amount: {best_prune_amount}\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Best prune amount found: {best_prune_amount} with acceptable accuracy loss.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruning with amount: 0.10\n",
      "Accuracy of the network on test images after Pruning: 93.5400%\n",
      "Sparsity in conv1.weight: 19.10%\n",
      "Sparsity in conv2.weight: 19.00%\n",
      "Sparsity in conv3.weight: 19.00%\n",
      "Sparsity in fc1.weight: 19.00%\n",
      "Sparsity in fc2.weight: 19.00%\n",
      "Sparsity in fc3.weight: 19.00%\n",
      "\n",
      "Pruning with amount: 0.15\n",
      "Accuracy of the network on test images after Pruning: 93.4800%\n",
      "Sparsity in conv1.weight: 31.25%\n",
      "Sparsity in conv2.weight: 31.15%\n",
      "Sparsity in conv3.weight: 31.15%\n",
      "Sparsity in fc1.weight: 31.15%\n",
      "Sparsity in fc2.weight: 31.15%\n",
      "Sparsity in fc3.weight: 31.17%\n",
      "\n",
      "Pruning with amount: 0.20\n",
      "Accuracy of the network on test images after Pruning: 93.2500%\n",
      "Sparsity in conv1.weight: 45.14%\n",
      "Sparsity in conv2.weight: 44.92%\n",
      "Sparsity in conv3.weight: 44.92%\n",
      "Sparsity in fc1.weight: 44.92%\n",
      "Sparsity in fc2.weight: 44.92%\n",
      "Sparsity in fc3.weight: 44.92%\n",
      "\n",
      "Pruning with amount: 0.25\n",
      "Accuracy of the network on test images after Pruning: 92.8500%\n",
      "Sparsity in conv1.weight: 59.03%\n",
      "Sparsity in conv2.weight: 58.69%\n",
      "Sparsity in conv3.weight: 58.69%\n",
      "Sparsity in fc1.weight: 58.69%\n",
      "Sparsity in fc2.weight: 58.69%\n",
      "Sparsity in fc3.weight: 58.67%\n",
      "\n",
      "Pruning with amount: 0.30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_7481/3634911342.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mpruned_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mapply_pruning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprune_amount\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# Create a copy if you need the original model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0mpruned_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 60\u001B[0;31m     \u001B[0mcurrent_accuracy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_and_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpruned_model\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlabels_map\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mprune_amount\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     61\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mbest_accuracy\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mcurrent_accuracy\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0maccuracy_threshold\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_7481/3634911342.py\u001B[0m in \u001B[0;36mtrain_and_evaluate\u001B[0;34m(model, train_loader, test_loader, device, labels_map, prune_amount)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# loop over the dataset multiple times\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mrunning_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m             \u001B[0;31m# get the inputs; data is a list of [inputs, labels]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m             \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    433\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    434\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 435\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    436\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    437\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    473\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 475\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    476\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    477\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 106\u001B[0;31m             \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_transform\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m             \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, tensor)\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mNormalized\u001B[0m \u001B[0mTensor\u001B[0m \u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m         \"\"\"\n\u001B[0;32m--> 226\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minplace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    227\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001B[0m in \u001B[0;36mnormalize\u001B[0;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0minplace\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 273\u001B[0;31m         \u001B[0mtensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    274\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    275\u001B[0m     \u001B[0mdtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T07:47:44.787649Z",
     "start_time": "2024-11-16T07:47:44.783875Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNISTModel(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1152, out_features=1000, bias=True)\n",
      "  (drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=1000, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:45:15.178238Z",
     "start_time": "2024-11-15T17:45:15.161935Z"
    }
   },
   "cell_type": "code",
   "source": "check_sparsity(model)\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 35.07%\n",
      "Sparsity in conv2.weight: 35.00%\n",
      "Sparsity in conv3.weight: 35.00%\n",
      "Sparsity in fc1.weight: 35.00%\n",
      "Sparsity in fc2.weight: 35.00%\n",
      "Sparsity in fc3.weight: 35.00%\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "# quantization  utils"
   ],
   "metadata": {
    "id": "YIR4hF5zJq7b"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# tmp test \n",
    "import torch\n",
    "\n",
    "def linear_quantize(input, scale, zero_point, inplace=False):\n",
    "    \"\"\"\n",
    "    Quantize single-precision input tensor to integers with the given scaling factor and zeropoint.\n",
    "    input: single-precision input tensor to be quantized\n",
    "    scale: scaling factor for quantization\n",
    "    zero_point: shift for quantization\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape scale and zero_point for convolutional weights and activations\n",
    "    if len(input.shape) == 4:\n",
    "        scale = scale.view(-1, 1, 1, 1)\n",
    "        zero_point = zero_point.view(-1, 1, 1, 1)\n",
    "    # reshape scale and zero_point for linear weights\n",
    "    elif len(input.shape) == 2:\n",
    "        scale = scale.view(-1, 1)\n",
    "        zero_point = zero_point.view(-1, 1)\n",
    "\n",
    "    # mapping single-precision input to integer values with the given scale and zero_point\n",
    "    if inplace:\n",
    "        input.mul_(scale).sub_(zero_point).round_()  # In-place quantization: input * scale - zero_point, then round\n",
    "        return input\n",
    "    else:\n",
    "        quantized_input = (input * scale - zero_point).round()  # Non in-place quantization\n",
    "        return quantized_input\n",
    "import torch\n",
    "\n",
    "# Example input tensor\n",
    "input_tensor = torch.randn(1, 3, 224, 224) # Example 4D tensor \n",
    "scale = torch.tensor([0.5])\n",
    "zero_point = torch.tensor([10])\n",
    "\n",
    "# In-place quantization\n",
    "quantized_tensor = linear_quantize(input_tensor, scale, zero_point, inplace=True)\n",
    "print(\"In-place quantized tensor:\", quantized_tensor)\n",
    "print(\"Original tensor (modified):\", input_tensor)  # input_tensor is modified\n",
    "\n",
    "# Non in-place quantization\n",
    "input_tensor = torch.randn(1, 3, 224, 224) # Re-initialize for demonstration\n",
    "quantized_tensor = linear_quantize(input_tensor, scale, zero_point, inplace=False)\n",
    "print(\"Non in-place quantized tensor:\", quantized_tensor)\n",
    "print(\"Original tensor (unchanged):\", input_tensor) # input_tensor remains unchanged"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn import Module, Parameter\n",
    "import sys\n",
    "import math\n",
    "from torch.autograd import Function, Variable\n",
    "import torch\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "\n",
    "def clamp(input, min, max, inplace=False):\n",
    "    \"\"\"\n",
    "    Clamp tensor input to (min, max).\n",
    "    input: input tensor to be clamped\n",
    "   \"\"\"\n",
    "    if inplace:\n",
    "        input.clamp_(min, max)  # In-place modification\n",
    "        return input\n",
    "    else:\n",
    "        return torch.clamp(input, min, max)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linear_quantize(input, scale, zero_point, inplace=False):\n",
    "    \"\"\"\n",
    "    Quantize single-precision input tensor to integers with the given scaling factor and zeropoint.\n",
    "    input: single-precision input tensor to be quantized\n",
    "    scale: scaling factor for quantization\n",
    "    zero_pint: shift for quantization\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # reshape scale and zeropoint for convolutional weights and activation\n",
    "    if len(input.shape) == 4:\n",
    "        scale = scale.view(-1, 1, 1, 1)\n",
    "        zero_point = zero_point.view(-1, 1, 1, 1)\n",
    "    # reshape scale and zeropoint for linear weights\n",
    "    elif len(input.shape) == 2:\n",
    "        scale = scale.view(-1, 1)\n",
    "        zero_point = zero_point.view(-1, 1)\n",
    "    # mapping single-precision input to integer values with the given scale and zeropoint\n",
    "    #print(\"mammmmmmm\")\n",
    "    if inplace:\n",
    "        \n",
    "\t    \n",
    "        input.mul_(scale).add_(zero_point).round_()\n",
    "        return input\n",
    "    else:\n",
    "        quantized_input = (input * scale + zero_point).round()  # Non in-place quantization\n",
    "        return quantized_input\n",
    "\t\t    \n",
    "\n",
    "\n",
    "def linear_dequantize(input, scale, zero_point, inplace=False):\n",
    "    \"\"\"\n",
    "    Map integer input tensor to fixed point float point with given scaling factor and zeropoint.\n",
    "    input: integer input tensor to be mapped\n",
    "    scale: scaling factor for quantization\n",
    "    zero_pint: shift for quantization\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape scale and zeropoint for convolutional weights and activation\n",
    "    if len(input.shape) == 4:\n",
    "        scale = scale.view(-1, 1, 1, 1)\n",
    "        zero_point = zero_point.view(-1, 1, 1, 1)\n",
    "    # reshape scale and zeropoint for linear weights\n",
    "    elif len(input.shape) == 2:\n",
    "        scale = scale.view(-1, 1)\n",
    "        zero_point = zero_point.view(-1, 1)\n",
    "    # mapping integer input to fixed point float point value with given scaling factor and zeropoint\n",
    "    if inplace:\n",
    "        input.sub_(zero_point).div_(scale)  # In-place dequantization\n",
    "        return input\n",
    "    else:\n",
    "        return (input - zero_point) / scale  # Non in-place dequantization\n",
    "\n",
    "\n",
    "def asymmetric_linear_quantization_params(num_bits,\n",
    "                                          saturation_min,\n",
    "                                          saturation_max,\n",
    "                                          integral_zero_point=True,\n",
    "                                          signed=True):\n",
    "    \"\"\"\n",
    "    Compute the scaling factor and zeropoint with the given quantization range.\n",
    "    saturation_min: lower bound for quantization range\n",
    "    saturation_max: upper bound for quantization range\n",
    "    \"\"\"\n",
    "    n = 2**num_bits - 1\n",
    "    scale = n / torch.clamp((saturation_max - saturation_min), min=1e-8)\n",
    "    zero_point = scale * saturation_min\n",
    "\n",
    "    if integral_zero_point:\n",
    "        if isinstance(zero_point, torch.Tensor):\n",
    "            zero_point = zero_point.round()\n",
    "        else:\n",
    "            zero_point = float(round(zero_point))\n",
    "    if signed:\n",
    "        zero_point += 2**(num_bits - 1)\n",
    "    return scale, zero_point\n",
    "\n",
    "\n",
    "class AsymmetricQuantFunction(Function):\n",
    "    \"\"\"\n",
    "    Class to quantize the given floating-point values with given range and bit-setting.\n",
    "    Currently only support inference, but not support back-propagation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, k, x_min=None, x_max=None):\n",
    "        \"\"\"\n",
    "        x: single-precision value to be quantized\n",
    "        k: bit-setting for x\n",
    "        x_min: lower bound for quantization range\n",
    "        x_max=None\n",
    "        \"\"\"\n",
    "\n",
    "        # if x_min is None or x_max is None or (sum(x_min == x_max) == 1\n",
    "        #                                       and x_min.numel() == 1):\n",
    "        #     x_min, x_max = x.min(), x.max()\n",
    "        scale, zero_point = asymmetric_linear_quantization_params(\n",
    "            k, x_min, x_max)\n",
    "        new_quant_x = linear_quantize(x, scale, zero_point, inplace=False)\n",
    "        n = 2**(k - 1)\n",
    "        new_quant_x = torch.clamp(new_quant_x, -n, n - 1)\n",
    "        quant_x = linear_dequantize(new_quant_x,\n",
    "                                    scale,\n",
    "                                    zero_point,\n",
    "                                    inplace=False)\n",
    "        return torch.autograd.Variable(quant_x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None, None, None\n",
    "\n",
    "\n",
    "class Quant_Linear(Module):\n",
    "\t\"\"\"\n",
    "\tClass to quantize given linear layer weights\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, weight_bit, full_precision_flag=False):\n",
    "\t\t\"\"\"\n",
    "\t\tweight: bit-setting for weight\n",
    "\t\tfull_precision_flag: full precision or not\n",
    "\t\trunning_stat: determines whether the activation range is updated or froze\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Quant_Linear, self).__init__()\n",
    "\t\tself.full_precision_flag = full_precision_flag\n",
    "\t\tself.weight_bit = weight_bit\n",
    "\t\tself.weight_function = AsymmetricQuantFunction.apply\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\ts = super(Quant_Linear, self).__repr__()\n",
    "\t\ts = \"(\" + s + \" weight_bit={}, full_precision_flag={})\".format(\n",
    "\t\t\tself.weight_bit, self.full_precision_flag)\n",
    "\t\treturn s\n",
    "\n",
    "\tdef set_param(self, linear):\n",
    "\t\tself.in_features = linear.in_features\n",
    "\t\tself.out_features = linear.out_features\n",
    "\t\tself.weight = Parameter(linear.weight.data.clone())\n",
    "\t\ttry:\n",
    "\t\t\tself.bias = Parameter(linear.bias.data.clone())\n",
    "\t\texcept AttributeError:\n",
    "\t\t\tself.bias = None\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tusing quantized weights to forward activation x\n",
    "\t\t\"\"\"\n",
    "\t\tw = self.weight\n",
    "\t\tx_transform = w.data.detach()\n",
    "\t\tw_min = x_transform.min(dim=1).values\n",
    "\t\tw_max = x_transform.max(dim=1).values\n",
    "\t\tif not self.full_precision_flag:\n",
    "\t\t\tw = self.weight_function(self.weight, self.weight_bit, w_min,w_max)\n",
    "\t\telse:\n",
    "\t\t\tw = self.weight\n",
    "\t\treturn F.linear(x, weight=w, bias=self.bias)\n",
    "\n",
    "\n",
    "class Quant_Conv2d(Module):\n",
    "\t\"\"\"\n",
    "\tClass to quantize given convolutional layer weights\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, weight_bit, full_precision_flag=False):\n",
    "\t\tsuper(Quant_Conv2d, self).__init__()\n",
    "\t\tself.full_precision_flag = full_precision_flag\n",
    "\t\tself.weight_bit = weight_bit\n",
    "\t\tself.weight_function = AsymmetricQuantFunction.apply\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\ts = super(Quant_Conv2d, self).__repr__()\n",
    "\t\ts = \"(\" + s + \" weight_bit={}, full_precision_flag={})\".format(\n",
    "\t\t\tself.weight_bit, self.full_precision_flag)\n",
    "\t\treturn s\n",
    "\n",
    "\tdef set_param(self, conv):\n",
    "\t\tself.in_channels = conv.in_channels\n",
    "\t\tself.out_channels = conv.out_channels\n",
    "\t\tself.kernel_size = conv.kernel_size\n",
    "\t\tself.stride = conv.stride\n",
    "\t\tself.padding = conv.padding\n",
    "\t\tself.dilation = conv.dilation\n",
    "\t\tself.groups = conv.groups\n",
    "\t\tself.weight = Parameter(conv.weight.data.clone())\n",
    "\t\ttry:\n",
    "\t\t\tself.bias = Parameter(conv.bias.data.clone())\n",
    "\t\texcept AttributeError:\n",
    "\t\t\tself.bias = None\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tusing quantized weights to forward activation x\n",
    "\t\t\"\"\"\n",
    "\t\tw = self.weight\n",
    "\t\tx_transform = w.data.contiguous().view(self.out_channels, -1)\n",
    "\t\tw_min = x_transform.min(dim=1).values\n",
    "\t\tw_max = x_transform.max(dim=1).values\n",
    "\t\tif not self.full_precision_flag:\n",
    "\t\t\tw = self.weight_function(self.weight, self.weight_bit, w_min,\n",
    "\t\t\t                         w_max)\n",
    "\t\telse:\n",
    "\t\t\tw = self.weight\n",
    "\n",
    "\t\treturn F.conv2d(x, w, self.bias, self.stride, self.padding,\n",
    "\t\t                self.dilation, self.groups)\n",
    "\n",
    "class QuantAct(Module):\n",
    "\t\"\"\"\n",
    "\tClass to quantize given activations\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t             activation_bit,\n",
    "\t             full_precision_flag=False,\n",
    "\t             running_stat=True,\n",
    "\t\t\t\t beta=0.9):\n",
    "\t\t\"\"\"\n",
    "\t\tactivation_bit: bit-setting for activation\n",
    "\t\tfull_precision_flag: full precision or not\n",
    "\t\trunning_stat: determines whether the activation range is updated or froze\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(QuantAct, self).__init__()\n",
    "\t\tself.activation_bit = activation_bit\n",
    "\t\tself.full_precision_flag = full_precision_flag\n",
    "\t\tself.running_stat = running_stat\n",
    "\t\tself.register_buffer('x_min', torch.zeros(1))\n",
    "\t\tself.register_buffer('x_max', torch.zeros(1))\n",
    "\t\tself.register_buffer('beta', torch.Tensor([beta]))\n",
    "\t\tself.register_buffer('beta_t', torch.ones(1))\n",
    "\t\tself.act_function = AsymmetricQuantFunction.apply\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"{0}(activation_bit={1}, full_precision_flag={2}, running_stat={3}, Act_min: {4:.2f}, Act_max: {5:.2f})\".format(\n",
    "\t\t\tself.__class__.__name__, self.activation_bit,\n",
    "\t\t\tself.full_precision_flag, self.running_stat, self.x_min.item(),\n",
    "\t\t\tself.x_max.item())\n",
    "\n",
    "\tdef fix(self):\n",
    "\t\t\"\"\"\n",
    "\t\tfix the activation range by setting running stat\n",
    "\t\t\"\"\"\n",
    "\t\tself.running_stat = False\n",
    "\n",
    "\tdef unfix(self):\n",
    "\t\t\"\"\"\n",
    "\t\tfix the activation range by setting running stat\n",
    "\t\t\"\"\"\n",
    "\t\tself.running_stat = True\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tquantize given activation x\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tif self.running_stat:\n",
    "\t\t\tx_min = x.data.min()\n",
    "\t\t\tx_max = x.data.max()\n",
    "\t\t\t# in-place operation used on multi-gpus\n",
    "\t\t\tself.x_min += -self.x_min + min(self.x_min, x_min)\n",
    "\t\t\tself.x_max += -self.x_max + max(self.x_max, x_max)\n",
    "\n",
    "\t\tif not self.full_precision_flag:\n",
    "\t\t\tquant_act = self.act_function(x, self.activation_bit, self.x_min,\n",
    "\t\t\t                              self.x_max)\n",
    "\t\t\treturn quant_act\n",
    "\t\telse:\n",
    "\t\t\treturn x\n",
    "\n",
    "\n",
    "class QuantActPreLu(Module):\n",
    "\t\"\"\"\n",
    "\tClass to quantize given activations\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\t\t\t act_bit,\n",
    "\t\t\t\t full_precision_flag=False,\n",
    "\t\t\t\t running_stat=True):\n",
    "\t\t\"\"\"\n",
    "\t\tactivation_bit: bit-setting for activation\n",
    "\t\tfull_precision_flag: full precision or not\n",
    "\t\trunning_stat: determines whether the activation range is updated or froze\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(QuantActPreLu, self).__init__()\n",
    "\t\tself.activation_bit = act_bit\n",
    "\t\tself.full_precision_flag = full_precision_flag\n",
    "\t\tself.running_stat = running_stat\n",
    "\t\tself.act_function = AsymmetricQuantFunction.apply\n",
    "\t\tself.quantAct=QuantAct(activation_bit=act_bit,running_stat=True)\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\ts = super(QuantActPreLu, self).__repr__()\n",
    "\t\ts = \"(\" + s + \" activation_bit={}, full_precision_flag={})\".format(\n",
    "\t\t\tself.activation_bit, self.full_precision_flag)\n",
    "\t\treturn s\n",
    "\n",
    "\tdef set_param(self, prelu):\n",
    "\t\tself.weight = Parameter(prelu.weight.data.clone())\n",
    "\n",
    "\n",
    "\tdef fix(self):\n",
    "\t\t\"\"\"\n",
    "\t\tfix the activation range by setting running stat\n",
    "\t\t\"\"\"\n",
    "\t\tself.running_stat = False\n",
    "\n",
    "\tdef unfix(self):\n",
    "\t\t\"\"\"\n",
    "\t\tfix the activation range by setting running stat\n",
    "\t\t\"\"\"\n",
    "\t\tself.running_stat = True\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tw = self.weight\n",
    "\t\tx_transform = w.data.detach()\n",
    "\t\ta_min = x_transform.min(dim=0).values\n",
    "\t\ta_max = x_transform.max(dim=0).values\n",
    "\t\tif not self.full_precision_flag:\n",
    "\t\t\tw = self.act_function(self.weight, self.activation_bit, a_min,\n",
    "\t\t\t\t\t\t\t\t\t a_max)\n",
    "\t\telse:\n",
    "\t\t\tw = self.weight\n",
    "\n",
    "\t\tx= F.prelu(x,weight=w)\n",
    "\t\tx=self.quantAct(x)\n",
    "\t\treturn x"
   ],
   "metadata": {
    "id": "jOOnsiDwJufB",
    "ExecuteTime": {
     "end_time": "2024-11-16T10:42:13.177467Z",
     "start_time": "2024-11-16T10:42:13.150643Z"
    }
   },
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "source": [
    "# quantization"
   ],
   "metadata": {
    "id": "Gay1rB3fmNAQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import io\n",
    "from collections import OrderedDict\n",
    "\n",
    "def quantize_model(model, weight_bit=None, act_bit=None):\n",
    "    \"\"\"\n",
    "    Recursively quantize a pretrained single-precision model to int8 quantized model\n",
    "    model: pretrained single-precision model\n",
    "    \"\"\"\n",
    "    # If model is Conv2d, Linear, or activation layer, apply quantization as specified\n",
    "    if type(model) == nn.Conv2d:\n",
    "        quant_mod = Quant_Conv2d(weight_bit=weight_bit)\n",
    "        quant_mod.set_param(model)\n",
    "        \n",
    "        return quant_mod\n",
    "    elif type(model) == nn.Linear:\n",
    "        quant_mod = Quant_Linear(weight_bit=weight_bit)\n",
    "        quant_mod.set_param(model)\n",
    "        return quant_mod\n",
    "    elif type(model) == nn.PReLU:\n",
    "        quant_mod = QuantActPreLu(act_bit=act_bit)\n",
    "        quant_mod.set_param(model)\n",
    "        return quant_mod\n",
    "    elif type(model) in {nn.ReLU, nn.ReLU6, nn.PReLU, nn.SiLU}:\n",
    "        return nn.Sequential(*[model, QuantAct(activation_bit=act_bit)])\n",
    "    elif type(model) == nn.Sequential or isinstance(model, nn.Sequential):\n",
    "        # Recursively quantize children within Sequential blocks\n",
    "        mods = OrderedDict()\n",
    "        for n, m in model.named_children():\n",
    "            if isinstance(m, Depth_Wise) and m.residual:\n",
    "                mods[n] = nn.Sequential(\n",
    "                    *[quantize_model(m, weight_bit=weight_bit, act_bit=act_bit), QuantAct(activation_bit=act_bit)]\n",
    "                )\n",
    "            else:\n",
    "                mods[n] = quantize_model(m, weight_bit=weight_bit, act_bit=act_bit)\n",
    "        return nn.Sequential(mods)\n",
    "    else:\n",
    "        # Serialize and reload the model to avoid deepcopy issues\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(model, buffer)\n",
    "        buffer.seek(0)\n",
    "        q_model = torch.load(buffer)\n",
    "\n",
    "        # Quantize other submodules\n",
    "        for attr in dir(model):\n",
    "            mod = getattr(model, attr)\n",
    "            if isinstance(mod, nn.Module) and 'norm' not in attr:\n",
    "                setattr(q_model, attr, quantize_model(mod, weight_bit=weight_bit, act_bit=act_bit))\n",
    "\n",
    "        return q_model\n"
   ],
   "metadata": {
    "id": "y31d_Qcqkitl",
    "ExecuteTime": {
     "end_time": "2024-11-16T11:12:05.062197Z",
     "start_time": "2024-11-16T11:12:05.055225Z"
    }
   },
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T11:12:52.322998Z",
     "start_time": "2024-11-16T11:12:52.315456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# retrain model after quantization function declaration \n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def retrain_after_quantization(q_model, train_loader, test_loader, num_epochs=10, learning_rate=1e-4, device='cpu'):\n",
    "    \"\"\"\n",
    "    Retrains (fine-tunes) a model *after* post-training quantization.\n",
    "\n",
    "    Args:\n",
    "        q_model: The quantized model.\n",
    "        train_loader: DataLoader for the training dataset.\n",
    "        test_loader, num_epochs, learning_rate, device: (as before)\n",
    "    \"\"\"\n",
    "\n",
    "    q_model.to(device)\n",
    "\n",
    "    # It's crucial to only train the *unquantized* parts of the model.\n",
    "    # This usually means BatchNorm layers and any remaining full-precision layers.\n",
    "    trainable_params = [param for param in q_model.parameters() if param.requires_grad]\n",
    "    optimizer = optim.Adam(trainable_params, lr=learning_rate)  # Adam is usually a good choice\n",
    "    criterion = nn.CrossEntropyLoss() # Or your appropriate loss function\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        q_model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():  # Use autocast for mixed precision (optional but helpful)\n",
    "                outputs = q_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Updates only the trainable parameters\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:\n",
    "                print(f\"[Epoch {epoch + 1}, Mini-batch {i + 1}] Loss: {running_loss / 200:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    return q_model"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# loading pruned model from filesystem \n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "new_model = FashionMNISTModel()\n",
    "state_dict = torch.load(\"/home/oem/Desktop/uni/ut/acc92.2200pruned0.40.pth\")\n",
    "\n",
    "pruned_weights = {}\n",
    "for key, value in state_dict.items():\n",
    "    if key.endswith(\"_orig\"):\n",
    "        pruned_weights[key[:-5]] = value * state_dict[key[:-5] + \"_mask\"]\n",
    "new_model.load_state_dict(pruned_weights, strict=False) # strict=False is crucial\n",
    "\n",
    "new_model.to(device)\n",
    "\n",
    "check_sparsity(new_model)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T15:25:22.005458Z",
     "start_time": "2024-11-16T15:12:21.918535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# stage 5,6 (quantization of pruned model based on weight_bit=act_bit =2,4,6,8\n",
    "\n",
    "bit_widths = [2, 4, 6, 8]  # Bit widths to experiment with\n",
    "\n",
    "for bit_width in bit_widths:\n",
    "    print(f\"Quantizing and retraining with bit width: {bit_width}\")\n",
    "\n",
    "    q_model = quantize_model(new_model, bit_width, bit_width)  # Quantize the model\n",
    "    retrained_model = retrain_after_quantization(q_model, train_loader, test_loader, device=device, learning_rate=1e-5, num_epochs=10) # Adjust num_epochs as needed\n",
    "\n",
    "\n",
    "    # Evaluate the retrained quantized model\n",
    "    retrained_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = retrained_model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy for bit width {bit_width}: {accuracy:.2f}%\")\n",
    "\n",
    "    # Save the retrained quantized model with clear filenames\n",
    "    model_filename = f\"retrained_quantized_model_w{bit_width}_a{bit_width}.pth\"\n",
    "    torch.save(retrained_model.state_dict(), model_filename)\n",
    "    print(f\"Saved model to {model_filename}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing and retraining with bit width: 2\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=2, full_precision_flag=False)\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=2, full_precision_flag=False)\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=2, full_precision_flag=False)\n",
      "[Epoch 1, Mini-batch 200] Loss: 0.019\n",
      "[Epoch 1, Mini-batch 400] Loss: 0.024\n",
      "[Epoch 1, Mini-batch 600] Loss: 0.020\n",
      "[Epoch 1, Mini-batch 800] Loss: 0.017\n",
      "[Epoch 2, Mini-batch 200] Loss: 0.016\n",
      "[Epoch 2, Mini-batch 400] Loss: 0.013\n",
      "[Epoch 2, Mini-batch 600] Loss: 0.013\n",
      "[Epoch 2, Mini-batch 800] Loss: 0.013\n",
      "[Epoch 3, Mini-batch 200] Loss: 0.010\n",
      "[Epoch 3, Mini-batch 400] Loss: 0.012\n",
      "[Epoch 3, Mini-batch 600] Loss: 0.011\n",
      "[Epoch 3, Mini-batch 800] Loss: 0.009\n",
      "[Epoch 4, Mini-batch 200] Loss: 0.008\n",
      "[Epoch 4, Mini-batch 400] Loss: 0.010\n",
      "[Epoch 4, Mini-batch 600] Loss: 0.008\n",
      "[Epoch 4, Mini-batch 800] Loss: 0.008\n",
      "[Epoch 5, Mini-batch 200] Loss: 0.007\n",
      "[Epoch 5, Mini-batch 400] Loss: 0.008\n",
      "[Epoch 5, Mini-batch 600] Loss: 0.007\n",
      "[Epoch 5, Mini-batch 800] Loss: 0.008\n",
      "[Epoch 6, Mini-batch 200] Loss: 0.008\n",
      "[Epoch 6, Mini-batch 400] Loss: 0.006\n",
      "[Epoch 6, Mini-batch 600] Loss: 0.007\n",
      "[Epoch 6, Mini-batch 800] Loss: 0.006\n",
      "[Epoch 7, Mini-batch 200] Loss: 0.005\n",
      "[Epoch 7, Mini-batch 400] Loss: 0.008\n",
      "[Epoch 7, Mini-batch 600] Loss: 0.006\n",
      "[Epoch 7, Mini-batch 800] Loss: 0.006\n",
      "[Epoch 8, Mini-batch 200] Loss: 0.006\n",
      "[Epoch 8, Mini-batch 400] Loss: 0.007\n",
      "[Epoch 8, Mini-batch 600] Loss: 0.008\n",
      "[Epoch 8, Mini-batch 800] Loss: 0.008\n",
      "[Epoch 9, Mini-batch 200] Loss: 0.007\n",
      "[Epoch 9, Mini-batch 400] Loss: 0.005\n",
      "[Epoch 9, Mini-batch 600] Loss: 0.006\n",
      "[Epoch 9, Mini-batch 800] Loss: 0.004\n",
      "[Epoch 10, Mini-batch 200] Loss: 0.005\n",
      "[Epoch 10, Mini-batch 400] Loss: 0.005\n",
      "[Epoch 10, Mini-batch 600] Loss: 0.006\n",
      "[Epoch 10, Mini-batch 800] Loss: 0.006\n",
      "Test Accuracy for bit width 2: 69.89%\n",
      "Saved model to retrained_quantized_model_w2_a2.pth\n",
      "Quantizing and retraining with bit width: 4\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=4, full_precision_flag=False)\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=4, full_precision_flag=False)\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=4, full_precision_flag=False)\n",
      "[Epoch 1, Mini-batch 200] Loss: 0.008\n",
      "[Epoch 1, Mini-batch 400] Loss: 0.006\n",
      "[Epoch 1, Mini-batch 600] Loss: 0.005\n",
      "[Epoch 1, Mini-batch 800] Loss: 0.005\n",
      "[Epoch 2, Mini-batch 200] Loss: 0.003\n",
      "[Epoch 2, Mini-batch 400] Loss: 0.003\n",
      "[Epoch 2, Mini-batch 600] Loss: 0.004\n",
      "[Epoch 2, Mini-batch 800] Loss: 0.003\n",
      "[Epoch 3, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 3, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 3, Mini-batch 600] Loss: 0.003\n",
      "[Epoch 3, Mini-batch 800] Loss: 0.003\n",
      "[Epoch 4, Mini-batch 200] Loss: 0.004\n",
      "[Epoch 4, Mini-batch 400] Loss: 0.003\n",
      "[Epoch 4, Mini-batch 600] Loss: 0.002\n",
      "[Epoch 4, Mini-batch 800] Loss: 0.003\n",
      "[Epoch 5, Mini-batch 200] Loss: 0.001\n",
      "[Epoch 5, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 5, Mini-batch 600] Loss: 0.003\n",
      "[Epoch 5, Mini-batch 800] Loss: 0.004\n",
      "[Epoch 6, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 6, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 6, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 6, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 7, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 7, Mini-batch 400] Loss: 0.001\n",
      "[Epoch 7, Mini-batch 600] Loss: 0.003\n",
      "[Epoch 7, Mini-batch 800] Loss: 0.003\n",
      "[Epoch 8, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 8, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 8, Mini-batch 600] Loss: 0.002\n",
      "[Epoch 8, Mini-batch 800] Loss: 0.001\n",
      "[Epoch 9, Mini-batch 200] Loss: 0.003\n",
      "[Epoch 9, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 9, Mini-batch 600] Loss: 0.002\n",
      "[Epoch 9, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 10, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 10, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 10, Mini-batch 600] Loss: 0.002\n",
      "[Epoch 10, Mini-batch 800] Loss: 0.003\n",
      "Test Accuracy for bit width 4: 89.68%\n",
      "Saved model to retrained_quantized_model_w4_a4.pth\n",
      "Quantizing and retraining with bit width: 6\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=6, full_precision_flag=False)\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=6, full_precision_flag=False)\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=6, full_precision_flag=False)\n",
      "[Epoch 1, Mini-batch 200] Loss: 0.005\n",
      "[Epoch 1, Mini-batch 400] Loss: 0.004\n",
      "[Epoch 1, Mini-batch 600] Loss: 0.004\n",
      "[Epoch 1, Mini-batch 800] Loss: 0.003\n",
      "[Epoch 2, Mini-batch 200] Loss: 0.003\n",
      "[Epoch 2, Mini-batch 400] Loss: 0.003\n",
      "[Epoch 2, Mini-batch 600] Loss: 0.002\n",
      "[Epoch 2, Mini-batch 800] Loss: 0.003\n",
      "[Epoch 3, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 3, Mini-batch 400] Loss: 0.001\n",
      "[Epoch 3, Mini-batch 600] Loss: 0.002\n",
      "[Epoch 3, Mini-batch 800] Loss: 0.003\n",
      "[Epoch 4, Mini-batch 200] Loss: 0.003\n",
      "[Epoch 4, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 4, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 4, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 5, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 5, Mini-batch 400] Loss: 0.001\n",
      "[Epoch 5, Mini-batch 600] Loss: 0.002\n",
      "[Epoch 5, Mini-batch 800] Loss: 0.003\n",
      "[Epoch 6, Mini-batch 200] Loss: 0.001\n",
      "[Epoch 6, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 6, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 6, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 7, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 7, Mini-batch 400] Loss: 0.003\n",
      "[Epoch 7, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 7, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 8, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 8, Mini-batch 400] Loss: 0.001\n",
      "[Epoch 8, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 8, Mini-batch 800] Loss: 0.001\n",
      "[Epoch 9, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 9, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 9, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 9, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 10, Mini-batch 200] Loss: 0.003\n",
      "[Epoch 10, Mini-batch 400] Loss: 0.001\n",
      "[Epoch 10, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 10, Mini-batch 800] Loss: 0.001\n",
      "Test Accuracy for bit width 6: 90.97%\n",
      "Saved model to retrained_quantized_model_w6_a6.pth\n",
      "Quantizing and retraining with bit width: 8\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=8, full_precision_flag=False)\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=8, full_precision_flag=False)\n",
      "conv2 quant_mode : (Quant_Conv2d() weight_bit=8, full_precision_flag=False)\n",
      "[Epoch 1, Mini-batch 200] Loss: 0.004\n",
      "[Epoch 1, Mini-batch 400] Loss: 0.003\n",
      "[Epoch 1, Mini-batch 600] Loss: 0.003\n",
      "[Epoch 1, Mini-batch 800] Loss: 0.003\n",
      "[Epoch 2, Mini-batch 200] Loss: 0.003\n",
      "[Epoch 2, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 2, Mini-batch 600] Loss: 0.003\n",
      "[Epoch 2, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 3, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 3, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 3, Mini-batch 600] Loss: 0.002\n",
      "[Epoch 3, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 4, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 4, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 4, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 4, Mini-batch 800] Loss: 0.001\n",
      "[Epoch 5, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 5, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 5, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 5, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 6, Mini-batch 200] Loss: 0.001\n",
      "[Epoch 6, Mini-batch 400] Loss: 0.002\n",
      "[Epoch 6, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 6, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 7, Mini-batch 200] Loss: 0.001\n",
      "[Epoch 7, Mini-batch 400] Loss: 0.001\n",
      "[Epoch 7, Mini-batch 600] Loss: 0.002\n",
      "[Epoch 7, Mini-batch 800] Loss: 0.001\n",
      "[Epoch 8, Mini-batch 200] Loss: 0.001\n",
      "[Epoch 8, Mini-batch 400] Loss: 0.001\n",
      "[Epoch 8, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 8, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 9, Mini-batch 200] Loss: 0.002\n",
      "[Epoch 9, Mini-batch 400] Loss: 0.001\n",
      "[Epoch 9, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 9, Mini-batch 800] Loss: 0.002\n",
      "[Epoch 10, Mini-batch 200] Loss: 0.001\n",
      "[Epoch 10, Mini-batch 400] Loss: 0.001\n",
      "[Epoch 10, Mini-batch 600] Loss: 0.001\n",
      "[Epoch 10, Mini-batch 800] Loss: 0.001\n",
      "Test Accuracy for bit width 8: 90.95%\n",
      "Saved model to retrained_quantized_model_w8_a8.pth\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:34:22.223712Z",
     "start_time": "2024-11-16T13:34:18.637837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# precision and f1-score and recall report base on pruned and quantized model (8bit)\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = retrained_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())  # Move predictions to CPU and convert to NumPy\n",
    "        all_labels.extend(labels.cpu().numpy())  # Move labels to CPU and convert to NumPy\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(all_labels, all_predictions, target_names=labels_map.values())\n",
    "print(report)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     T-Shirt       0.89      0.88      0.88      1000\n",
      "     Trouser       0.99      0.99      0.99      1000\n",
      "    Pullover       0.90      0.90      0.90      1000\n",
      "       Dress       0.94      0.94      0.94      1000\n",
      "        Coat       0.88      0.92      0.90      1000\n",
      "      Sandal       0.99      0.98      0.98      1000\n",
      "       Shirt       0.81      0.78      0.79      1000\n",
      "     Sneaker       0.95      0.99      0.97      1000\n",
      "         Bag       0.99      0.99      0.99      1000\n",
      "  Ankle Boot       0.98      0.96      0.97      1000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T15:47:01.657606Z",
     "start_time": "2024-11-16T15:47:01.648930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# analyzing and comparing quantized pruned model and basic model \n",
    "def analyze_model(model, quantized_model=None):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1. Number of neurons per layer\n",
    "    results[\"neurons_per_layer\"] = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            results[\"neurons_per_layer\"][name] = module.out_features if isinstance(module, nn.Linear) else module.out_channels\n",
    "\n",
    "\n",
    "    # 2. Size in bits (KB)\n",
    "    def get_size_in_kb(model_to_analyze):\n",
    "        total_bits = 0\n",
    "        for param in model_to_analyze.parameters():\n",
    "            total_bits += torch.numel(param) * torch.finfo(param.dtype).bits\n",
    "        total_kb = (total_bits / 8) / 1024  # Convert bits to KB\n",
    "        return total_kb\n",
    "    def get_size_int8_in_kb(model_to_analyze):\n",
    "        total_bits = 0\n",
    "        for param in model_to_analyze.parameters():\n",
    "            total_bits += torch.numel(param) * 8\n",
    "        total_kb = (total_bits / 8) / 1024  # Convert bits to KB\n",
    "        return total_kb\n",
    "    results[\"basic_model_size_kb\"] = get_size_in_kb(model)\n",
    "\n",
    "    if quantized_model:\n",
    "        results[\"quantized_model_size_kb\"] = get_size_int8_in_kb(quantized_model)\n",
    "\n",
    "        # 3. Compression ratio\n",
    "        results[\"compression_ratio\"] = results[\"basic_model_size_kb\"] / results[\"quantized_model_size_kb\"]\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (assuming you have a model named 'model' and a quantized version 'quantized_model'):\n",
    "analysis_results = analyze_model(model, quantized_model=retrained_model)\n",
    "\n",
    "for key, value in analysis_results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons_per_layer: {'conv1': 32, 'conv2': 64, 'conv3': 128, 'fc1': 1000, 'fc2': 120, 'fc3': 10}\n",
      "basic_model_size_kb: 5341.6015625\n",
      "quantized_model_size_kb: 1335.400390625\n",
      "compression_ratio: 4.0\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
